{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15079,
     "status": "ok",
     "timestamp": 1647603105941,
     "user": {
      "displayName": "Heiko Karus",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17229837771439642165"
     },
     "user_tz": -60
    },
    "id": "5_qPrWxJIH1I",
    "outputId": "1456ffa6-0353-4355-cfbc-620f6722d5a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "#Colab Path:\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\",force_remount = False)\n",
    "my_path = \"/Colab_Notebooks/data/training_data/\"\n",
    "gdrive_path = \"/content/drive/My Drive\" + my_path\n",
    "# Dateipfad für Training-, Valdidierung- und Test-Dateien\n",
    "TRAIN_DATASET_PATH = gdrive_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "SGWJ0XS5IueR"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel\n",
    "from matplotlib import colors\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "#from keras.layers import Input, merge, Convolution3D, MaxPooling3D, UpSampling3D\n",
    "from keras.layers import Dropout, GaussianNoise\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras import backend as K\n",
    "#from keras.layers import Input, merge, Conv2D, MaxPooling2D, UpSampling2D, concatenate\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate\n",
    "from keras.layers import Dropout\n",
    "from keras.losses import mean_absolute_error\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "#from keras.layers.merge import concatenate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import callbacks\n",
    "import h5py\n",
    "from keras.callbacks import CSVLogger\n",
    "import keras\n",
    "import pandas as pd\n",
    "from skimage import exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "CAfsSaUCVKOM"
   },
   "outputs": [],
   "source": [
    "#adding another TRAIN_DATASET_PATH when code is not excecuted on Google Colab\n",
    "\n",
    "if 'TRAIN_DATASET_PATH' not in locals():\n",
    "    TRAIN_DATASET_PATH = \"../../data/training_data/\"\n",
    "\n",
    "N_PATIENTS=25\n",
    "TRAIN_FILE=\"MenisKI21_Training_\"+str(N_PATIENTS)+\"_6_Classes.h5\"\n",
    "TEST_FILE=\"MenisKI21_Test_\"+str(N_PATIENTS)+\"_6_Classes.h5\"\n",
    "PREDICTION_FILE=\"MenisKI21_Prediction_\"+str(N_PATIENTS)+\"_6_Classes.h5\"\n",
    "# Parameter\n",
    "VOLUME_SLICES_PER_BATCH = 16\n",
    "IMG_SIZE=384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "7nQrV6lSGqMg"
   },
   "outputs": [],
   "source": [
    "smoothDSC = np.finfo(float).eps\n",
    "\n",
    "# DICE Coeffizient für 3 Klassen Segmentierung\n",
    "def dice_coef(y_true, y_pred, smooth=smoothDSC):\n",
    "    class_num = 7\n",
    "    for i in range(class_num):\n",
    "        y_true_f = K.flatten(y_true[:,:,:,i])\n",
    "        y_pred_f = K.flatten(y_pred[:,:,:,i])\n",
    "        intersection = K.sum(y_true_f * y_pred_f)\n",
    "        loss = ((2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth))\n",
    "        if i == 0:\n",
    "            total_loss = loss\n",
    "        else:\n",
    "            total_loss = total_loss + loss\n",
    "    total_loss = total_loss / class_num\n",
    "    return total_loss\n",
    " \n",
    "# Inspiriert durch https://github.com/keras-team/keras/issues/9395\n",
    "\n",
    "def dice_coef_medial(y_true, y_pred,epsilon=np.finfo(float).eps):\n",
    "    y_true_f = K.flatten(y_true[:,:,:,5])\n",
    "    y_pred_f = K.flatten(y_pred[:,:,:,5])\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + epsilon) / (K.sum(y_true_f) + K.sum(y_pred_f) + epsilon)\n",
    "\n",
    "def dice_coef_lateral(y_true, y_pred,epsilon=np.finfo(float).eps):\n",
    "    y_true_f = K.flatten(y_true[:,:,:,6])\n",
    "    y_pred_f = K.flatten(y_pred[:,:,:,6])\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + epsilon) / (K.sum(y_true_f) + K.sum(y_pred_f) + epsilon)\n",
    "\n",
    "def dice_coef_tibia(y_true, y_pred,epsilon=np.finfo(float).eps):\n",
    "    y_true_f = K.flatten(y_true[:,:,:,3])\n",
    "    y_pred_f = K.flatten(y_pred[:,:,:,3])\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + epsilon) / (K.sum(y_true_f) + K.sum(y_pred_f) + epsilon)\n",
    "\n",
    "def dice_coef_femur(y_true, y_pred,epsilon=np.finfo(float).eps):\n",
    "    y_true_f = K.flatten(y_true[:,:,:,1])\n",
    "    y_pred_f = K.flatten(y_pred[:,:,:,1])\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + epsilon) / (K.sum(y_true_f) + K.sum(y_pred_f) + epsilon)\n",
    "\n",
    "def dice_coef_C_tibia(y_true, y_pred,epsilon=np.finfo(float).eps):\n",
    "    y_true_f = K.flatten(y_true[:,:,:,4])\n",
    "    y_pred_f = K.flatten(y_pred[:,:,:,4])\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + epsilon) / (K.sum(y_true_f) + K.sum(y_pred_f) + epsilon)\n",
    "\n",
    "def dice_coef_C_femur(y_true, y_pred,epsilon=np.finfo(float).eps):\n",
    "    y_true_f = K.flatten(y_true[:,:,:,2])\n",
    "    y_pred_f = K.flatten(y_pred[:,:,:,2])\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + epsilon) / (K.sum(y_true_f) + K.sum(y_pred_f) + epsilon)\n",
    "\n",
    "# Präzsion\n",
    "def precision(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "# Sensitivität   \n",
    "def sensitivity(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    return true_positives / (possible_positives + K.epsilon())\n",
    "\n",
    "\n",
    "# Spezifität\n",
    "def specificity(y_true, y_pred):\n",
    "    true_negatives = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))\n",
    "    possible_negatives = K.sum(K.round(K.clip(1-y_true, 0, 1)))\n",
    "    return true_negatives / (possible_negatives + K.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "DoGNY0r5I1Xy"
   },
   "outputs": [],
   "source": [
    "# Erstelle UNet Segmentierungsmodell (nicht kompiliert)\n",
    "def build_unet(inputs, ker_init, dropout):\n",
    "    conv1 = Conv2D(32, (5, 5), padding=\"same\", activation=\"relu\", kernel_initializer = ker_init)(inputs)\n",
    "    D1=Dropout(dropout)(conv1)\n",
    "    conv1 = Conv2D(32, (5, 5), padding=\"same\", activation=\"relu\", kernel_initializer = ker_init)(D1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Conv2D(64, (5, 5), padding=\"same\", activation=\"relu\", kernel_initializer = ker_init)(pool1)\n",
    "    D2=Dropout(dropout)(conv2)\n",
    "    conv2 = Conv2D(64, (5, 5), padding=\"same\", activation=\"relu\", kernel_initializer = ker_init)(D2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Conv2D(128, (5, 5), padding=\"same\", activation=\"relu\", kernel_initializer = ker_init)(pool2)\n",
    "    D3=Dropout(dropout)(conv3)\n",
    "    conv3 = Conv2D(128, (5, 5), padding=\"same\", activation=\"relu\", kernel_initializer = ker_init)(D3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv5 = Conv2D(256, (5, 5), padding=\"same\", activation=\"relu\", kernel_initializer = ker_init)(pool3)\n",
    "    D5=Dropout(dropout)(conv5)\n",
    "    conv5 = Conv2D(256, (5, 5), padding=\"same\", activation=\"relu\", kernel_initializer = ker_init)(D5)\n",
    "\n",
    "    up7 = concatenate([UpSampling2D(size=(2, 2))(conv5), conv3], axis=-1)\n",
    "    conv7 = Conv2D(128, (5, 5), padding=\"same\", activation=\"relu\", kernel_initializer = ker_init)(up7)\n",
    "    D7=Dropout(dropout)(conv7)\n",
    "    conv7 = Conv2D(128, (5, 5), padding=\"same\", activation=\"relu\", kernel_initializer = ker_init)(D7)\n",
    "\n",
    "    up8 = concatenate([UpSampling2D(size=(2, 2))(conv7), conv2], axis=-1)\n",
    "    conv8 = Conv2D(64, (5, 5), padding=\"same\", activation=\"relu\", kernel_initializer = ker_init)(up8)\n",
    "    D8=Dropout(dropout)(conv8)\n",
    "    conv8 = Conv2D(64, (5, 5), padding=\"same\", activation=\"relu\", kernel_initializer = ker_init)(D8)\n",
    "\n",
    "    up9 = concatenate([UpSampling2D(size=(2, 2))(conv8), conv1], axis=-1)\n",
    "    conv9 = Conv2D(32, (5, 5), padding=\"same\", activation=\"relu\", kernel_initializer = ker_init)(up9)\n",
    "    D9=Dropout(dropout)(conv9)\n",
    "    conv9 = Conv2D(32, (5, 5), padding=\"same\", activation=\"relu\", kernel_initializer = ker_init)(D9)\n",
    "\n",
    "    conv10 = Conv2D(3, (1, 1), activation=\"softmax\")(conv9)\n",
    "\n",
    "    return Model(inputs = inputs, outputs = conv10)\n",
    "\n",
    "# Füge Eingabeschicht hinzu\n",
    "input_layer = Input((IMG_SIZE, IMG_SIZE,1))\n",
    "\n",
    "# Erstelle UNet Modell\n",
    "model = build_unet(input_layer, 'he_normal', 0.15)\n",
    "\n",
    "# Kompiliere UNet Modell mit categorical_crossentropy als Kostenfunktion, Adam als Optimierer und verschiedene Metriken\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=0.0005), metrics = ['accuracy',tf.keras.metrics.MeanIoU(num_classes=7), dice_coef, dice_coef_medial, dice_coef_lateral , precision, sensitivity, specificity] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "XyM90iVa0lVz"
   },
   "outputs": [],
   "source": [
    "# Erstelle Listen mit Keys für Training, Validierung und Test. Nur String pro Datensatz -> Endung \"_dcm\" bzw. \"_seg\" wird entfernt\n",
    "with h5py.File(TRAIN_DATASET_PATH+TRAIN_FILE, 'r') as hf:\n",
    "  train_ids= [s[:-4] for s in list(hf.keys()) if \"Training\" in s.split(\"_\") and \"dcm\" in s.split(\"_\")]\n",
    "  validation_ids= [s[:-4] for s in list(hf.keys()) if \"Validation\" in s.split(\"_\") and \"dcm\" in s.split(\"_\")]\n",
    "\n",
    "#with h5py.File(TRAIN_DATASET_PATH+TEST_FILE, 'r') as hf:\n",
    "  #test_ids= [s[:-4] for s in list(hf.keys()) if \"Test\" in s.split(\"_\") and \"dcm\" in s.split(\"_\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "c-ZT6WuXLswk"
   },
   "outputs": [],
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    'Erzeugt Daten für Keras'\n",
    "    def __init__(self, list_IDs, filepath, slices=VOLUME_SLICES_PER_BATCH, dim=(IMG_SIZE,IMG_SIZE), batch_size = 1, n_channels = 1, shuffle=True):\n",
    "        'Initialisierung'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        self.filepath=filepath\n",
    "        self.slices=slices\n",
    "\n",
    "    def __len__(self):\n",
    "        'Bezeichnet die Anzahl der Batches pro Epoche'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Erzeuge eine Batch von Daten'\n",
    "        # Erzeugen von Indizes für Batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Liste der IDs finden\n",
    "        Batch_ids = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Daten generieren\n",
    "        X, y = self.__data_generation(Batch_ids)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Aktualisiert Indizes nach jeder Epoche'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, Batch_ids):\n",
    "        'Erzeugt Daten, die Stichproben von batch_size enthalten' # X : (n_samples, *dim, n_channels); Y : (n_samples, *dim, n_classes=3)\n",
    "        # Initialisierung\n",
    "        X = np.zeros((self.batch_size*self.slices, *self.dim, self.n_channels))\n",
    "        Y = np.zeros((self.batch_size*self.slices, *self.dim, 7))\n",
    "\n",
    "        with h5py.File(self.filepath, 'r') as hf:\n",
    "        # Daten generieren\n",
    "          for c, i in enumerate(Batch_ids):        \n",
    "            X[self.slices*c:self.slices*(c+1),:,:,:] = np.reshape(hf[i+'_dcm'][:],(self.slices,IMG_SIZE, IMG_SIZE,1))\n",
    "            Y[self.slices*c:self.slices*(c+1),:,:,:] = to_categorical(hf[i+'_seg'][:], num_classes=7)\n",
    "\n",
    "\n",
    "\n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "OO-RAddBRJZd"
   },
   "outputs": [],
   "source": [
    "# Erstelle DataGenerator für Training, Validierung und Test\n",
    "training_generator = DataGenerator(train_ids,TRAIN_DATASET_PATH+TRAIN_FILE)\n",
    "valid_generator = DataGenerator(validation_ids,TRAIN_DATASET_PATH+TRAIN_FILE)\n",
    "#test_generator = DataGenerator(test_ids,TRAIN_DATASET_PATH+TEST_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 3556590,
     "status": "error",
     "timestamp": 1647606973558,
     "user": {
      "displayName": "Heiko Karus",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17229837771439642165"
     },
     "user_tz": -60
    },
    "id": "HG5YGVc3LpmX",
    "outputId": "92eee647-e919-44ee-fda3-533c626220bf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pydev debugger: warning: trying to add breakpoint to file that does not exist: c:\\users\\max\\pycharmprojects\\kniesegmentierung\\python_code\\unet_training_6_classes.ipynb (will have no effect)\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Setze Keras-Session zurück\n",
    "K.clear_session()\n",
    "\n",
    "# Data Logger für Training\n",
    "csv_logger = CSVLogger(TRAIN_DATASET_PATH+'training_6_Classes.log', separator=',', append=False)\n",
    "\n",
    "# Rückgaben für Training -> EarlyStopping um Training abzubrechen, falls keine Verbesserung mehr stattfindet und ReduceLROnPlateau, um Lernrate zu reduzieren\n",
    "callbacks = [keras.callbacks.EarlyStopping(monitor='loss', min_delta=0, patience=4, verbose=1, mode='auto'),\n",
    "             keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5,patience=2, min_lr=0.000001, verbose=1),\n",
    "             csv_logger]\n",
    "\n",
    "# Trainiere UNet\n",
    "history =  model.fit(training_generator,\n",
    "                    epochs=50,\n",
    "                    steps_per_epoch=len(train_ids),\n",
    "                    validation_data = valid_generator,\n",
    "                    callbacks= callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-16 06:58:34.181728\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "print(now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "sRSrurmw19AZ"
   },
   "outputs": [],
   "source": [
    "# Speichere Modell\n",
    "model.save(TRAIN_DATASET_PATH+\"model_multiclass_25_6_Classes.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "executionInfo": {
     "elapsed": 1083,
     "status": "ok",
     "timestamp": 1647607298697,
     "user": {
      "displayName": "Heiko Karus",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17229837771439642165"
     },
     "user_tz": -60
    },
    "id": "28Tr0xvOUJlc",
    "outputId": "3ba6f4e2-ad4d-42e0-c243-64bacb0e077a"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/training_data/training.log'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [14], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Plotte Trainingsparameter\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m hist \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mTRAIN_DATASET_PATH\u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtraining.log\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msep\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m,\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mengine\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mpython\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m acc\u001B[38;5;241m=\u001B[39mhist[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m      5\u001B[0m val_acc\u001B[38;5;241m=\u001B[39mhist[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval_accuracy\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001B[0m, in \u001B[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    209\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    210\u001B[0m         kwargs[new_arg_name] \u001B[38;5;241m=\u001B[39m new_arg_value\n\u001B[1;32m--> 211\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001B[0m, in \u001B[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(args) \u001B[38;5;241m>\u001B[39m num_allow_args:\n\u001B[0;32m    326\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m    327\u001B[0m         msg\u001B[38;5;241m.\u001B[39mformat(arguments\u001B[38;5;241m=\u001B[39m_format_argument_list(allow_args)),\n\u001B[0;32m    328\u001B[0m         \u001B[38;5;167;01mFutureWarning\u001B[39;00m,\n\u001B[0;32m    329\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39mfind_stack_level(),\n\u001B[0;32m    330\u001B[0m     )\n\u001B[1;32m--> 331\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001B[0m, in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001B[0m\n\u001B[0;32m    935\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[0;32m    936\u001B[0m     dialect,\n\u001B[0;32m    937\u001B[0m     delimiter,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    946\u001B[0m     defaults\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelimiter\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m\"\u001B[39m},\n\u001B[0;32m    947\u001B[0m )\n\u001B[0;32m    948\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[1;32m--> 950\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001B[0m, in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    602\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[0;32m    604\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[1;32m--> 605\u001B[0m parser \u001B[38;5;241m=\u001B[39m TextFileReader(filepath_or_buffer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m    607\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[0;32m    608\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[1;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[0;32m   1439\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m   1441\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles: IOHandles \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1442\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[1;34m(self, f, engine)\u001B[0m\n\u001B[0;32m   1733\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[0;32m   1734\u001B[0m         mode \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m-> 1735\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1736\u001B[0m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1737\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1738\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1739\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcompression\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1740\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmemory_map\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1741\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_text\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1742\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding_errors\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstrict\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1743\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstorage_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1744\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1745\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1746\u001B[0m f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles\u001B[38;5;241m.\u001B[39mhandle\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\common.py:856\u001B[0m, in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    851\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m    852\u001B[0m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[0;32m    853\u001B[0m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[0;32m    854\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mencoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mmode:\n\u001B[0;32m    855\u001B[0m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[1;32m--> 856\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m    857\u001B[0m \u001B[43m            \u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    858\u001B[0m \u001B[43m            \u001B[49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    859\u001B[0m \u001B[43m            \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    860\u001B[0m \u001B[43m            \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    861\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnewline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    862\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    863\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    864\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[0;32m    865\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(handle, ioargs\u001B[38;5;241m.\u001B[39mmode)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '../data/training_data/training.log'"
     ]
    }
   ],
   "source": [
    "# Plotte Trainingsparameter\n",
    "hist = pd.read_csv(TRAIN_DATASET_PATH+'training.log', sep=',', engine='python')\n",
    "\n",
    "acc=hist['accuracy']\n",
    "val_acc=hist['val_accuracy']\n",
    "\n",
    "epoch=range(len(acc))\n",
    "\n",
    "loss=hist['loss']\n",
    "val_loss=hist['val_loss']\n",
    "\n",
    "train_dice=hist['dice_coef']\n",
    "val_dice=hist['val_dice_coef']\n",
    "\n",
    "f,ax=plt.subplots(1,4,figsize=(16,8))\n",
    "\n",
    "ax[0].plot(epoch,acc,'b',label='Training Accuracy')\n",
    "ax[0].plot(epoch,val_acc,'r',label='Validation Accuracy')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(epoch,loss,'b',label='Training Loss')\n",
    "ax[1].plot(epoch,val_loss,'r',label='Validation Loss')\n",
    "ax[1].legend()\n",
    "\n",
    "ax[2].plot(epoch,train_dice,'b',label='Training dice coef')\n",
    "ax[2].plot(epoch,val_dice,'r',label='Validation dice coef')\n",
    "ax[2].legend()\n",
    "\n",
    "ax[3].plot(epoch,hist['mean_io_u'],'b',label='Training mean IOU')\n",
    "ax[3].plot(epoch,hist['val_mean_io_u'],'r',label='Validation mean IOU')\n",
    "ax[3].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 41922,
     "status": "ok",
     "timestamp": 1645357937450,
     "user": {
      "displayName": "Heiko Karus",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17229837771439642165"
     },
     "user_tz": -60
    },
    "id": "voPnXkVMW_VF",
    "outputId": "ffb1cd00-f372-4b75-d386-03b8f5fb7434"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 41s 409ms/step - loss: 0.0011 - accuracy: 0.9996 - mean_io_u: 0.9834 - dice_coef: 0.9277 - dice_coef_medial: 0.8898 - dice_coef_lateral: 0.8936 - precision: 0.9996 - sensitivity: 0.9996 - specificity: 0.9998\n"
     ]
    }
   ],
   "source": [
    "# Test (Evaluation) des UNets\n",
    "results = model.evaluate(test_generator, batch_size=1)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "UNET_Training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
